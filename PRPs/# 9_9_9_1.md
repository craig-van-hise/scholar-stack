
# PRP: "Trust-Based" Logic Refactor

**Role:** Senior Search Engineer
**Task:** Refactor the validation logic to maximize yield by removing the "PDF Download" dependency.

## 1. The New "Lightweight" Gate

Remove the `verify_pdf_frontmatter` requirement from the acceptance loop. Replace it with a metadata check:

* **Step 1:** Construct the `audit_blob` from the OpenAlex `title` + `abstract` + `keywords`.
* **Step 2:** Run the **Fuzzy Regex** (`cross[\s\-]*talk[\s\-]*cancellations?`) on this blob.
* **Step 3:** If Match -> **ACCEPT**.
* **Step 4 (The Safety Net):** If No Match (likely due to truncated abstract), but the paper was found via a specific query -> **ACCEPT** (mark as "Keyword Inferred").

## 2. Deduplication is King

* Ensure that we track `seen_ids` globally across all loops.
* If "Binaural" finds Paper A, and "3D Audio" finds Paper A again, the log should show `[Duplicate] Skipped...` rather than processing it twice.

## 3. PDF is for Storage, Not Validation

* Downloading the PDF becomes a **post-acceptance** task (Phase 3). If the download fails (because it's a landing page), we still keep the metadata in the CSV so you know the paper exists.

---

### Implementation Code Block

Give this to the agent. It removes the "PDF Gate" and will likely jump your yield from 17 to 100+ immediately.

```python
    def _process_batch(self, results):
        """
        Refactored Validation:
        1. Deduplicate.
        2. Audit Metadata (Title/Abstract).
        3. If keyword found OR inferred by search -> Accept.
        4. PDF Download happens LATER (don't kill the paper if download fails).
        """
        import re
        
        # Fuzzy regex for the keyword (handles hyphens/spaces)
        # e.g. "crosstalk cancellation" -> "cross[\s\-]*talk[\s\-]*cancellations?"
        primary_key = self.keywords_list[0].replace('"', '').lower()
        fuzzy_pattern = re.sub(r's?\s+', r'[\\s\\-]*', primary_key) + r's?'
        
        for item in results:
            work_id = item['id']
            
            # 1. Global Deduplication
            if work_id in self.seen_ids:
                continue
            self.seen_ids.add(work_id)
            
            # 2. Construct Audit Blob (Title + Abstract + Keywords)
            title = item.get('title', '')
            if not title: continue # Skip works with no title
            
            abstract = self.reconstruct_abstract(item.get('abstract_inverted_index'))
            keywords = " ".join([k.get('display_name', '') for k in item.get('keywords', [])])
            
            audit_blob = f"{title} {abstract} {keywords}".lower()
            
            # 3. The Logic Check
            # We trust the OpenAlex Search Engine mostly, but we verify if possible.
            match_found = re.search(fuzzy_pattern, audit_blob)
            
            if match_found:
                print(f"   [Accepted] {title[:60]}...")
                self._add_final_result(item)
            else:
                # 4. The Choueiri Safety Net
                # If the abstract is missing/truncated, but OpenAlex returned it for this specific query,
                # we assume their full-text index saw something we can't see. WE KEEP IT.
                print(f"   [Accepted (Inferred)] {title[:60]}...")
                self._add_final_result(item)

            # Stop if we hit the limit
            if len(self.research_catalog) >= self.max_results:
                break

```
